# Talos GCP Deployment Configuration (Reference)
# This file documents all available configuration variables.
# Usage: ./talos-gcp -c examples/talos.env deploy

# --- Cluster Identity ---
# Unique name for the cluster (used for resource naming: vpc, instances, etc.)
# Max 20 characters.
CLUSTER_NAME="talos-gcp-cluster"

# Region and Zone
REGION="us-central1"
ZONE="us-central1-b"

# --- Versions ---
# Talos Linux Version (Global default)
# Determines the OS image used (e.g., talos-v1-12-3-gcp-amd64)
TALOS_VERSION="v1.12.3"

# Kubernetes Tools Versions
KUBECTL_VERSION="v1.35.0"
HELM_VERSION="v3.16.2"

# Component Versions
CILIUM_VERSION="1.18.6"
TRAEFIK_VERSION="38.0.2" # Traefik Chart Version
ROOK_CHART_VERSION="v1.18.9"

# --- Control Plane Configuration ---
# Number of Control Plane Nodes (Allowed: 1, 3, 5...)
CP_COUNT=1
# Machine Type (e.g., e2-standard-2, n2-standard-4)
CP_MACHINE_TYPE="e2-standard-2"
# Disk Size (GB)
CP_DISK_SIZE="200GB"
# Service Account for Control Plane (Defaults to cluster SA)
# CP_SERVICE_ACCOUNT="cp-sa@project.iam.gserviceaccount.com"

# Override Talos Version for CP (Defaults to TALOS_VERSION)
# CP_TALOS_VERSION="$TALOS_VERSION"

# Extensions & Kernel Args (Comma-separated)
# extensions: e.g., "siderolabs/gvisor,siderolabs/nvidia-container-toolkit"
# kernel_args: e.g., "console=ttyS0,115200"
# CP_EXTENSIONS=""
# CP_KERNEL_ARGS=""

# --- Worker Configuration (Node Pools) ---
# Define pools using bash array. Default is ("worker").
# NODE_POOLS=("worker" "storage" "gpu")
NODE_POOLS=("worker")

# Per-Pool Configuration
# Format: POOL_{POOL_NAME_UPPERCASE}_{VAR_NAME}
# Hyphens in pool names are replaced by underscores (e.g. pool-1 -> POOL_POOL_1_...)

# > Pool: 'worker' (Default)
POOL_WORKER_COUNT=1
POOL_WORKER_TYPE="e2-standard-2"
POOL_WORKER_DISK_SIZE="200GB"
# POOL_WORKER_LABELS="role=worker,tier=frontend"
# POOL_WORKER_TAINTS="dedicated=web:NoSchedule"
# POOL_WORKER_EXTENSIONS=""
# POOL_WORKER_KERNEL_ARGS=""

# Custom Machine Type Example
# If TYPE="custom", you must specify VCPU and MEMORY_GB
# POOL_WORKER_TYPE="custom"
# POOL_WORKER_VCPU="4"
# POOL_WORKER_MEMORY_GB="16"
# POOL_WORKER_FAMILY="n2" # Optional (default: n2)

# Additional Disks (e.g. for Rook OSDs or Local PVs)
# Format: "type:size:device-name" (Space separated)
# POOL_WORKER_ADDITIONAL_DISKS="pd-ssd:100GB:fast-data pd-standard:500GB:slow-data"

# Secondary Network (Storage Network) Attachment
# Requires STORAGE_CIDR to be set (see Network Configuration)
# POOL_WORKER_USE_STORAGE_NET="false"

# --- Network Configuration ---
# VPC Name (Defaults to ${CLUSTER_NAME}-vpc)
# VPC_NAME="${CLUSTER_NAME}-vpc"

# Subnet CIDR for Nodes (Default: 10.100.0.0/20)
SUBNET_RANGE="10.100.0.0/20"

# Pod CIDR (Alias IPs for Native Routing) (Default: 10.200.0.0/14)
POD_CIDR="10.200.0.0/14"

# Service CIDR (ClusterIPs) (Default: 10.96.0.0/20)
SERVICE_CIDR="10.96.0.0/20"

# Google Cloud Health Check Source Ranges (Only change if using custom FW rules)
# HC_SOURCE_RANGES="35.191.0.0/16,130.211.0.0/22"

# Multi-NIC Storage Network (Optional)
# Separates Replication/Storage traffic onto a second nic.
# STORAGE_CIDR="172.16.0.0/24"
# CP_USE_STORAGE_NETWORK="false"

# --- CNI & Features ---
# Install Cilium CNI (Default: true)
INSTALL_CILIUM="true"
# Install Hubble Observability (Default: true)
INSTALL_HUBBLE="true"
# Install GCP Persistent Disk CSI Driver (Default: true)
INSTALL_CSI="true"

# Cilium Routing Mode
# "native" (Default): Uses VPC Alias IPs. High performance.
# "tunnel": Uses VXLAN/Geneve encapsulation.
CILIUM_ROUTING_MODE="native"

# --- Storage (Rook Ceph) ---
# Enable Rook Ceph Operator deployment (Default: false)
ROOK_ENABLE="false"

# Client Mode: Connect to an external Rook cluster
# If set, local OSDs are ignored and this cluster acts as a client.
# ROOK_EXTERNAL_CLUSTER_NAME="rook-ceph-central"

# Server Mode: Resource Requests (Recommended Defaults)
# ROOK_MDS_CPU="3"
# ROOK_MDS_MEMORY="4Gi"
# ROOK_OSD_CPU="1"
# ROOK_OSD_MEMORY="2Gi"

# --- Ingress ---
# Reserve static IP for Ingress LB
INGRESS_IP_COUNT=1
# Reuse specific IP Name (Optional)
# INGRESS_IPV4_CONFIG="my-static-ip-name"

# --- Firewall / Security ---
# Open Custom Ports on Workers (e.g. for NodePort or HostPort services)
# Format: "START-END" or "PORT" (Comma separated not fully supported, use range or single)
# WORKER_OPEN_TCP_PORTS="30000-32767"
# WORKER_OPEN_UDP_PORTS="30000-32767"
# Source CIDR for custom ports (Default: 0.0.0.0/0)
WORKER_OPEN_SOURCE_RANGES="0.0.0.0/0"

# --- Scheduling (Cost Savings) ---
# Automatically stop/start instance groups based on schedule.
# Start Time (HH:MM, 24h)
# WORK_HOURS_START="08:00"
# Stop Time (HH:MM, 24h)
# WORK_HOURS_STOP="18:00"
# Days (Default: Mon-Fri)
# WORK_HOURS_DAYS="Mon-Fri"
# Timezone (Defaults to auto-detect from Region)
# WORK_HOURS_TIMEZONE="Europe/Berlin"

# --- VPC Peering ---
# Array of other CLUSTER_NAMEs to peer with.
# Ensures firewall rules and routes are exchanged.
# PEER_WITH=("prod-db" "shared-services")

# --- Service Identity ---
# Service Account Name (Defaults to generic one based on cluster name)
# SA_NAME="my-cluster-sa"

# --- Bastion Host ---
# Bastion OS Image
BASTION_IMAGE_FAMILY="ubuntu-2404-lts-amd64"
BASTION_IMAGE_PROJECT="ubuntu-os-cloud"

# --- Labels ---
# GCP Resource Labels (Comma separated key=value)
# LABELS="environment=dev,owner=admin"

# --- Global Extensions/Kernel Args ---
# Applied to ALL nodes if not overridden by POOL_*
# POOL_EXTENSIONS=""
# POOL_KERNEL_ARGS=""
