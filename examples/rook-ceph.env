# Cluster Configuration
CLUSTER_NAME="rook-ceph"
REGION="europe-west3"
ZONE="europe-west3-a"

# Network
# Cluster Network (nic0) - Base Range (Organic)
SUBNET_RANGE="172.16.0.0/20"
POD_CIDR="172.20.0.0/14"
SERVICE_CIDR="172.30.0.0/20"

# Multi-NIC Storage (nic1) - Offset Range (Organic)
STORAGE_CIDR="172.16.16.0/24"
# NEW: Disable Storage Network on Control Plane (Security/Simplicity)
CP_USE_STORAGE_NETWORK="false"

# Control Plane Configuration
CP_COUNT=3

# Node Pools Configuration
# -------------------------
NODE_POOLS=("osd" "mon")

# Pool 1: osd (Storage/OSD Nodes)
# Maps to IG: rook-ceph-ig-osd
POOL_OSD_COUNT=5
POOL_OSD_TYPE="e2-standard-4"
POOL_OSD_USE_STORAGE_NET="true"
POOL_OSD_LABELS="role=ceph-osd"

# Additional Disks for OSDs
POOL_OSD_ADDITIONAL_DISKS="pd-ssd:1GB:disk-fast-1 pd-ssd:50GB:disk-fast-50 pd-standard:1TB:disk-slow-1tb"

# Rook Integration
ROOK_ENABLE="true"
ROOK_DATA_DEVICE_NAME="disk-slow-1tb"
ROOK_METADATA_DEVICE_NAME="disk-fast-50"

# Pool 2: mon (Ceph Monitor Nodes)
# Maps to IG: rook-ceph-ig-mon
POOL_MON_COUNT=3
POOL_MON_TYPE="e2-small"
POOL_MON_USE_STORAGE_NET="true"
POOL_MON_LABELS="role=ceph-mon"
POOL_MON_TAINTS="role=ceph-mon:NoSchedule"

# Small Cluster Resource Tuning (MDS & OSD)
ROOK_MDS_CPU="500m"
ROOK_MDS_MEMORY="1Gi"
ROOK_OSD_CPU="500m"
ROOK_OSD_MEMORY="1Gi"

# Custom Image Configuration
# --------------------------
# All nodes get extensions (Set as Generic Pool Defaults)
POOL_EXTENSIONS="siderolabs/amd-ucode,siderolabs/binfmt-misc,siderolabs/btrfs,siderolabs/qemu-guest-agent"
POOL_KERNEL_ARGS="console=ttyS0,115200"

# Specific overrides (Optional - Empty means inherit from POOL_*)
POOL_OSD_EXTENSIONS="" 
POOL_OSD_KERNEL_ARGS=""

POOL_MON_EXTENSIONS=""
POOL_MON_KERNEL_ARGS=""

INGRESS_IP_COUNT=2

# Custom Worker Ports (Ceph Monitors)
export WORKER_OPEN_TCP_PORTS="3300,6789"
CILIUM_ROUTING_MODE="native"
SERVICE_CIDR="172.30.0.0/20"
